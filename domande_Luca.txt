    MLP_mac.v
-. chi è che mette start a zero (nel ciclo successivo a quello in cui è stato messo a 1)?
    R: è in MLP.v che start viene modificato
1. in questo file manca il discorso del RESET, va aggiunto oppure qui ci basta lo 'start'?

    MLP_weight_mem.v
1. è verosimile che la lettura non rispetti il clock?
2. abbiamo il filo 'rst' ma non lo usiamo...

    MLP_layer.v
1. stiamo troncando da 64 bit a 16, non è un problema?
    
    MLP
1. il filo 'rst' attualmente già presente, cosa fa?
-. come avviene l'inizializzazione degli accumulatori?
    R: non c'è bisogno di inizializzarli (a zero per esempio), semplicemente al primo ciclo avrò start=1 e quel che accade è che acc <= il risultato della prima moltiplicazione, quindi OK.




-. si ma, poi effettivamente come leggo pesi e input?
1. se ora io volessi fare in modo che la MLP eseguisse più inferenze (in sequenza) come dovrei fare?
2. la width del MAC_accumulator è giusta 64?
3. ma quanti MAC ci riusciamo a mettere nella FPGA? Che nel caso alziamo il num di perceptron nell'hidden layer...
-. finché usiamo i numeri hardcoded nelle tb funziona ed ok, ma se usiamo i nostri numeri scritti nei txt siamo sicuri che le operazioni funzionano bene?
    R: la conversione non viene fatta come la pensa Luca, viene fatta tutta con la "stessa conversione" -> torna

    DA FARE:
- aggiusta dimensioni wire nel mlp.v X
- vedi con sinc a risultato negativo X
- oltre che i pesi, anche l'input deve esser letto da txt anziché hardcoded
- commenta mlp.v
- pulisci porgetto GitHub da file inutili
- capisci come Coco voleva fare la moltiplicazione

- bias da 1 a 1_fixed va messo nel mlp.v
- test statistico del training 

- testa con più neuroni
- rileggere documentazione
- guardare quanto codice togliere dalla documentazione